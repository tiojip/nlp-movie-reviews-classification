{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "208c5874-578a-4e23-b79c-df59c3624257",
   "metadata": {},
   "source": [
    "# Devoir 1 : La classification de deux façons avec les ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fed991f-42e1-461e-b70a-f0db4d4ace7a",
   "metadata": {},
   "source": [
    "Bienvenue à votre premier devoir! Quand vous le soumetterez sur ZoneCours, assurez-vous de renommer le fichier: **\"Devoir1_{prénom}_{nomdefamille}.ipynb\"**. Les instructions sont en français, par contre comme beaucoup des matériaux du cours sont en anglais, vous trouverez la traduction anglaise avec chaque problème. Votre devoir est due au plus tard à 11:59 PM le 20 février, 2025. Si vous avez travaillé avec un ou des collègues, écrivez leur nom ici:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab42b54-9ebd-4a55-abf8-30d78aae2d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "__autheur__ = \"{Leroy Tiojip}\"\n",
    "__collaborateurs__ = \"{Leur nom, si il y a plusieurs personnes séparez les par un point-virgule; sinon, à laissez vide}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b355af-393c-4b5b-9208-a33a6da1eff5",
   "metadata": {},
   "source": [
    "Toutes les librairies python dont vous aurez besoin sont ici. Si une librairie qui semble être utile est manquante, c'est par exprès pour que vous écriviez vos fonctions par vous même. Il ne faut pas modifier cette case, ni rajouter de libraires ailleurs. Par contre, vous pouvez utiliser toutes fonctions que vous trouvez utiles dans ces librairies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caa7eb50-167c-4444-b053-6784bfda57ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import math\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "random.seed(202401)\n",
    "torch.manual_seed(202401)\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58554253-49db-4fb2-9fcf-fcc6c61e5336",
   "metadata": {},
   "source": [
    "## Problème 1 [18 points]\n",
    "Dans ce problème, vous allez implémenter un classificateur Naive Bayes de critiques de films qui utilise des n-grams comme variables pour prédire si une critique est positive ou négative. Vous comparerez trois classificateurs différents qui utiliseront respectivement des  unigrams, bigrams et trigrams.\n",
    "\n",
    "*In this problem you will be implementing a movie review Naive Bayes classifier that uses n-gram features to predict whether a review was positive or negative. You will compare three different classifiers which will respectively use unigram features, bigram features and trigram features.* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d84310-fb29-4fe9-b9b6-3580192ab9d8",
   "metadata": {},
   "source": [
    "La première étape consiste à télécharger les données et à créer notre répartition test/train. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e548814d-0404-44e1-919a-455fab056c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_classes = movie_reviews.categories()\n",
    "pos_reviews = [(movie_reviews.raw(fileid), 'pos') for fileid in movie_reviews.fileids(categories=['pos'])]\n",
    "neg_reviews = [(movie_reviews.raw(fileid), 'neg') for fileid in movie_reviews.fileids(categories=['neg'])]\n",
    "train_dataset = pos_reviews[:800] + neg_reviews[:800]\n",
    "test_dataset = pos_reviews[800:] + neg_reviews[800:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d201d60c-8ded-49e8-867f-9e4c32ac6967",
   "metadata": {},
   "source": [
    "1.  **[3 points]** Vous allez maintenant devoir écrire une fonction de prétraitement de text qui prend une critique de film, supprime la ponctuation, met tous les mots en minuscules et les divise en tokens en fonction de l'espacement. Assurez-vous de ne pas avoir de tokens vides.\n",
    "\n",
    "    *You will now have to write a preprocessing function which takes a review, removes punctuation, lower cases all the words and splits them into tokens based on  any type of spacing. Make sure you do not have empty tokens.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2394ae28-2e2d-43b2-b76e-0b5ffcf2f01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(review:str):\n",
    "    tokenized_review = []\n",
    "    punctuations = list(string.punctuation)\n",
    "    ## TO DO\n",
    "    review = review.lower()\n",
    "    review = review.translate(str.maketrans('', '', string.punctuation.replace(\"'\", \"\")))\n",
    "    tokenized_review =re.findall(r'\\b\\w+\\b', review)\n",
    "    \n",
    "    ##\n",
    "    return tokenized_review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f2a065-3cb9-4767-ba99-9992bff502dd",
   "metadata": {},
   "source": [
    "2. **[3 points]** Afin d'extraire les n-grams représentatifs d'une classe, nous aurons besoin de quelques fonctions helpers. La première que vous devez implémenter est une fonction qui prend un texte tokenisé et un hyperparamètre *n* et retourne l'ensemble unique de ngrams présents dans le texte tokenisé, soit le vocabulaire. Notez que chaque ngram doit être de type tuple, et nous supposons qu'aucun padding n'est effectué.\n",
    "\n",
    "   *In order to extract the n-gram features of a class, we will need a couple helper functions. The first one you will have to implement is a function which takes a tokenized text and a hyperparameter *n* and returns the unique set of ngrams present in the tokenized text. Note that each ngram should be of type tuple, and we assume that no padding is done.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4840e246-cef9-4c52-97c0-4241b267c64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram_vocabulary(tokenized_reviews:list[tuple[list[str], str]], n:int):\n",
    "    ngram_vocab = []\n",
    "    ## TO DO\n",
    "    for tokens, label in tokenized_reviews: \n",
    "        if len(tokens) >= n:  \n",
    "            ngrams = [tuple(tokens[i:i+n]) for i in range(len(tokens) - n + 1)] \n",
    "            ngram_vocab.extend(ngrams)  \n",
    "\n",
    "\n",
    "    \n",
    "    ##\n",
    "    return ngram_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddefd3d-4ad5-4cad-b2ea-2885f74644ac",
   "metadata": {},
   "source": [
    "3. **[4 points]** Créez une fonction qui récupère les ngrams pour une classe particulière de textes dans un ensemble de données. Cette fonction doit d'abord (a) filtrer l'ensemble de textes tokenisés par la valeur du paramètre de classe, puis (b) extraire le modèle n-grams représentatif de la classe compte tenu du vocabulaire. Veillez à mettre en œuvre Laplace smoothing pour chaque modèle de ngrams. Une fois de plus, aucun padding n'est nécessaire.\n",
    "   \n",
    "   *Create a function that retrieves the ngram features for a particular class of texts in a dataset. This function should first (a) filter the tokenized texts dataset based on the class parameter, and then (b) extract the n-gram model representative of the class given the vocabulary. Make sure to implement Laplace smoothing for each ngram model. Once again, no padding is necessary.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9ffde07-0f99-45b4-8722-9c13594a93d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_ngram_features(tokenized_reviews:list[tuple[list[str], str]], review_class:str, ngram_vocab:list[tuple], vocab_size:int):\n",
    "    class_ngrams = {}\n",
    "    ## TO DO\n",
    "    filtered_reviews = [tokens for tokens, label in tokenized_reviews if label == review_class]\n",
    "    class_ngram_counts = Counter()\n",
    "    for tokens in filtered_reviews:\n",
    "        n = len(ngram_vocab[0])  # Taille des n-grammes \n",
    "        ngrams = [tuple(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]\n",
    "        class_ngram_counts.update(ngrams)\n",
    "\n",
    "    total_ngrams = sum(class_ngram_counts.values())  # Total des n-grammes de cette classe\n",
    "    for ngram in ngram_vocab:\n",
    "        class_ngrams[ngram] = (class_ngram_counts.get(ngram, 0) + 1) / (total_ngrams + vocab_size)\n",
    "\n",
    "    \n",
    "    ##\n",
    "    return class_ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491add63-af53-42af-b9fa-588588e24491",
   "metadata": {},
   "source": [
    "#### Classificateur Naive Bayes avec Ngram\n",
    "\n",
    "En utilisant toutes les fonctions helpers que vous venez d'implémenter, entraînons maintenant un classificateur NB en utilisant des unigram, bigram et trigram. \n",
    "Voici une fonction qui renvoie la probabilité logarithmique à priori d'une classe dans un ensemble de données, quel que soit le contenu de la classe.\n",
    "\n",
    "*Using all of the helper functions you just implemented, let's now train a NB classifier using unigram, bigram, and trigram features. \n",
    "Here is a helper function that returns the prior log likelihood of a particular class in a dataset irrespective of class content.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae8ce74a-f330-4609-9144-9f0d2ca02dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prior(tokenized_reviews:list[tuple[list[str], str]], review_class:str):\n",
    "    prob = 0.0\n",
    "    total = len(tokenized_reviews)\n",
    "    class_count = 0\n",
    "    for _ , c in tokenized_reviews:\n",
    "        if c == review_class:\n",
    "            class_count+=1\n",
    "    prob = class_count/total\n",
    "    return math.log(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38d913e-eca9-48f1-9898-f33c39595c3f",
   "metadata": {},
   "source": [
    "4. **[4 points]** Écrivez une fonction supplémentaire qui, étant donné un critique de film test, retourne la classe la plus probable en fonction de la distribution à priori sur les classes et de la distribution multinomiale de ngrams par classes. Notez que cette fonction doit ignorer tous les ngrams qui ne font pas partie du modèle de ngrams entraîné.\n",
    "\n",
    "   *Write one more helper function which given a test review returns its most likely review class conditioned on the prior distribution over classes and some ngram multinomial distribution over classes. Note, it should ignore any ngrams that are not part of the trained ngram model features.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5193c24-747f-4045-bb63-45684c398ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_prediction(review:list[str], ngram_multinomial_dist:dict[str, dict[str,float]], class_prior_dist:dict[str, float], n:int):\n",
    "    prediction = \"\"\n",
    "    ## TO DO\n",
    "    test_ngrams = [tuple(review[i:i+n]) for i in range(len(review) - n + 1)]\n",
    "    class_scores = {} \n",
    "    for review_class, prior_log_prob in class_prior_dist.items():\n",
    "        log_prob = prior_log_prob  \n",
    "        for ngram in test_ngrams:\n",
    "            if ngram in ngram_multinomial_dist[review_class]:  \n",
    "                log_prob += math.log(ngram_multinomial_dist[review_class][ngram])\n",
    "\n",
    "        class_scores[review_class] = log_prob\n",
    "\n",
    "    prediction = max(class_scores, key=class_scores.get)\n",
    "\n",
    "    \n",
    "    ##\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfe32aa-bb21-48ce-8db8-7a7d3fe8c0de",
   "metadata": {},
   "source": [
    "Et voici notre classificateur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed8a892f-91fc-4255-8a42-2b5b0368ae32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NB_ngram_classifier(train_dataset:list[tuple[str, str]], test_dataset:list[tuple[str, str]], review_classes:list[str]):\n",
    "    # 1: We preprocess both the test and train datasets\n",
    "    tokenized_test = [(preprocess(review), review_class) for review, review_class in  test_dataset]\n",
    "    tokenized_train = [(preprocess(review), review_class) for review, review_class in  train_dataset]\n",
    "    \n",
    "    # 2: Let's now extract the unigram, bigram, and trigram vocabularies we will use.\n",
    "    unigram_vocab = get_ngram_vocabulary(tokenized_train, 1)\n",
    "    bigram_vocab = get_ngram_vocabulary(tokenized_train, 2)\n",
    "    trigram_vocab = get_ngram_vocabulary(tokenized_train, 3)\n",
    "    \n",
    "    # 3: We must now *learn* the set of features for each category in the tokenized train data. We represent our features as a multinomial distributions over review_classes. \n",
    "    # We will compare three distributions, (a) unigram, (b) bigram, and (c) trigram multinomials. Additionally, we get the prior distribution over review classes.\n",
    "    unigram_multinomial_dist = dict()\n",
    "    bigram_multinomial_dist = dict()\n",
    "    trigram_multinomial_dist = dict()\n",
    "    class_prior_dist = dict()\n",
    "    for review_class in review_classes:\n",
    "        # n-gram multinomial\n",
    "        unigram_multinomial_dist[review_class] = get_class_ngram_features(tokenized_train, review_class, unigram_vocab, len(unigram_vocab))\n",
    "        bigram_multinomial_dist[review_class] = get_class_ngram_features(tokenized_train, review_class, bigram_vocab, len(unigram_vocab))\n",
    "        trigram_multinomial_dist[review_class] = get_class_ngram_features(tokenized_train, review_class, trigram_vocab, len(unigram_vocab))\n",
    "        # class prior\n",
    "        class_prior_dist[review_class] = get_prior(tokenized_train, review_class)\n",
    "        \n",
    "    # 4: Now that we have our class features by ngram model we will try to predict the class of each review in our tokenized test dataset. \n",
    "    results = []\n",
    "    for index, (tokenized_review, class_label) in enumerate(tokenized_test):\n",
    "        results.append({'review_id': index, \n",
    "                            'class_label':class_label, \n",
    "                            'unigram_prediction': get_class_prediction(tokenized_review, unigram_multinomial_dist, class_prior_dist, 1),\n",
    "                            'bigram_prediction': get_class_prediction(tokenized_review, bigram_multinomial_dist, class_prior_dist, 2),\n",
    "                            'trigram_prediction': get_class_prediction(tokenized_review, trigram_multinomial_dist, class_prior_dist, 3),\n",
    "                           })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc1a137f-fcff-48be-868f-eba7eb9dd46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should run for a minute or two.\n",
    "results = NB_ngram_classifier(train_dataset, test_dataset, review_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ec8276-54ef-4011-8d0e-2366f08df7a2",
   "metadata": {},
   "source": [
    "#### Résultats\n",
    "\n",
    "Comparons maintenant les performances relatives de nos classificateurs NB avec unigram, bigram et trigram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c1d70c-82c6-49b4-be8d-ef408ae527b9",
   "metadata": {},
   "source": [
    "5. **[4 points]** Écrivez une fonction qui retourne les scores moyens de précision, recall et F1, ainsi que le accuracy score moyen pour chaque classificateur.\n",
    "\n",
    "   *Write a function that returns the overall precision, recall, and balanced F1 scores across all classes as well as overall accuracy for each classifiers predictions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ea262de-bb33-4cee-ab36-29b61f7d29e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracies(results:dict):\n",
    "    accuracies = {'unigram':dict([('precision', 0.0),('recall', 0.0), ('f1', 0.0), ('accuracy', 0.0)]),\n",
    "                  'bigram':dict([('precision', 0.0),('recall', 0.0), ('f1', 0.0), ('accuracy', 0.0)]),\n",
    "                  'trigram':dict([('precision', 0.0),('recall', 0.0), ('f1', 0.0), ('accuracy', 0.0)])}\n",
    "    ## TO DO\n",
    "    metrics = {\n",
    "        'unigram': {'TP': 0, 'FP': 0, 'FN': 0},\n",
    "        'bigram': {'TP': 0, 'FP': 0, 'FN': 0},\n",
    "        'trigram': {'TP': 0, 'FP': 0, 'FN': 0}\n",
    "    }\n",
    "\n",
    "    for res in results:\n",
    "        true_label = res['class_label']\n",
    "        \n",
    "        for model in ['unigram', 'bigram', 'trigram']:\n",
    "            pred_label = res[f'{model}_prediction']\n",
    "            \n",
    "            if pred_label == true_label:\n",
    "                metrics[model]['TP'] += 1  \n",
    "            else:\n",
    "                metrics[model]['FP'] += 1  \n",
    "                metrics[model]['FN'] += 1  \n",
    "\n",
    "    for model in ['unigram', 'bigram', 'trigram']:\n",
    "        TP = metrics[model]['TP']\n",
    "        FP = metrics[model]['FP']\n",
    "        FN = metrics[model]['FN']\n",
    "        total = TP + FP + FN\n",
    "\n",
    "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "        recall = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "        f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        accuracy = TP / total if total > 0 else 0.0\n",
    "\n",
    "        accuracies[model]['precision'] = precision\n",
    "        accuracies[model]['recall'] = recall\n",
    "        accuracies[model]['f1'] = f1\n",
    "        accuracies[model]['accuracy'] = accuracy\n",
    "\n",
    "    \n",
    "    ##\n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61bdfde5-d9bd-4a88-ac71-c6dc32198477",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = get_accuracies(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "96302a0d-1290-407f-bb4a-6f7614f33523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>unigram</th>\n",
       "      <td>0.6800</td>\n",
       "      <td>0.6800</td>\n",
       "      <td>0.6800</td>\n",
       "      <td>0.515152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bigram</th>\n",
       "      <td>0.7975</td>\n",
       "      <td>0.7975</td>\n",
       "      <td>0.7975</td>\n",
       "      <td>0.663202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trigram</th>\n",
       "      <td>0.8250</td>\n",
       "      <td>0.8250</td>\n",
       "      <td>0.8250</td>\n",
       "      <td>0.702128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         precision  recall      f1  accuracy\n",
       "unigram     0.6800  0.6800  0.6800  0.515152\n",
       "bigram      0.7975  0.7975  0.7975  0.663202\n",
       "trigram     0.8250  0.8250  0.8250  0.702128"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(accuracies, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db88379d-c264-4b3a-b471-bff3c949db67",
   "metadata": {},
   "source": [
    "## Problème 2 [18 points]\n",
    "\n",
    "Nous allons maintenant essayer une autre forme de classificateur, la régression logistique. Vous allez mettre en œuvre un un simple réseau neuronal qui implémente la régression logistique et qui utilise des ngrams comme variables indépendantes, pour prédire la classe de critiques de films. Nous utiliserons les mêmes ensembles de données test et train que pour le problème 1. \n",
    "\n",
    "*We will now extend problem 1 and consider another form of classifier, logistic regression. You will implement a logistic regression classifier via a simple single layered neural network that uses ngrams as its features, or independent variables, to predict the class of reviews. We will use the same test and train datasets as problem 1.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc93d2fc-a01f-490f-a02b-85466208ed3e",
   "metadata": {},
   "source": [
    "1. **[3 points]** Écrivez une fonction qui retourne un vocabulaire de ngram pour n'importe quel *n*. Il est important que la taille du vocabulaire soit limitée à la taille maximale spécifiée. Il doit inclure les *max_vocab_size* ngrams les plus fréquents dans *tokenized_reviews*.\n",
    "\n",
    "   *Write a helper function that returns an ngram_vocabulary for any *n*. Importantly the vocabulary size should be limited to the maximum vocabulary size specified. It should include the *max_vocab_size* most frequent ngrams across all reviews.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d5e3d978-8b41-4b66-ad35-63ab9bd832ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram_vocabulary_maxsize(tokenized_reviews:list[tuple[list[str], str]], n:int, max_vocab_size:int):\n",
    "    ngram_vocab = []\n",
    "    ## TO DO\n",
    "    all_ngrams = []\n",
    "    for tokens, _ in tokenized_reviews:\n",
    "        ngrams = [tuple(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]\n",
    "        all_ngrams.extend(ngrams) \n",
    "        \n",
    "    ngram_counts = Counter(all_ngrams)\n",
    "    most_common_ngrams = [ngram for ngram, _ in ngram_counts.most_common(max_vocab_size)]\n",
    "    ngram_vocab = most_common_ngrams \n",
    "    \n",
    "    \n",
    "    ##\n",
    "    return ngram_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d374b531-4873-4ffa-91ea-0ef1122f860f",
   "metadata": {},
   "source": [
    "2. **[3 points]** Écrivez une fonction qui prend un vocabulaire de ngrams et une critique de film tokenisé et retourne une représentation vectorisée des comptes de ngrams dans la critique. Le type du vecteur retourné doit être un torch tensor de floats.\n",
    "\n",
    "   *Write a helper function which takes an ngram vocabulary and a tokenized review and returns a vectorized representation of the ngram counts in the review. The return type should be a torch tensor of floats.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "97a60bcd-7127-4f48-9dcb-9c3a4935582b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectorized_review(review:list[str], ngram_vocab:list[str]):\n",
    "    vectorized_review = torch.Tensor([0.0])\n",
    "    ## TO DO\n",
    "    n = len(ngram_vocab[0]) if ngram_vocab else 1 \n",
    "    review_ngrams = [tuple(review[i:i+n]) for i in range(len(review) - n + 1)]\n",
    "    vector = [review_ngrams.count(ngram) for ngram in ngram_vocab]\n",
    "    vectorized_review = torch.tensor(vector, dtype=torch.float32)\n",
    "    \n",
    "    \n",
    "    ##\n",
    "    return vectorized_review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f23c49-c826-42e0-be93-a75f9917e6cc",
   "metadata": {},
   "source": [
    "3. **[4 points]** Écrivez une classe python ReviewDataset qui implémente un objet torch Dataset. Elle doit prendre en input *tokenized_reviews* (une liste de tuples (tokenized_review, review_class)) ainsi qu'un vocabulaire de ngrams. Votre fonction d'initialisation doit vectoriser toutes les critiques, de sorte que self.vectorized_reviews soit un torch tensor, ou une matrice de taille |tokenized_reviews| x |ngram_vocab|. Quant à self.labels, il s'agit également d'un torch tensor de taille |tokenized_reviews| x |review_classes|, où chaque index est un vecteur one-hot tel que [1,0] est 'neg' et [0,1] est 'pos'.\n",
    "  \n",
    "   *Write a ReviewDataset class implements a torch Dataset object. It should take as input tokenized_reviews (a list of (tokenized_review, review_class) tuples) as well as a vocabulary of ngrams. Your initializing function should vectorize all reviews, such that self.vectorized_reviews is a torch tensor, or matrix of size |tokenized_reviews| x |ngram_vocab|. As for your self.labels, this should also be a torch tensor of size |tokenized_reviews| x |review_classes|, were each index is a one hot vector such that [1,0] is'neg' and [0,1] is 'pos' class labels.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "22eeefda-39b3-42c3-a184-42acc36c5eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenized_reviews:list[tuple[list[str], str]], ngram_vocab:list[tuple]):\n",
    "        vectorized_reviews = [] \n",
    "        labels = [] \n",
    "        ## TO DO\n",
    "        class_map = {'neg': [1, 0], 'pos': [0, 1]} \n",
    "        for tokens, label in tokenized_reviews:\n",
    "            vectorized_review = get_vectorized_review(tokens, ngram_vocab)\n",
    "            vectorized_reviews.append(vectorized_review)\n",
    "            labels.append(class_map[label][0])\n",
    "\n",
    "        \n",
    "        ##\n",
    "        self.vectorized_reviews = torch.stack(vectorized_reviews, dim=0)\n",
    "        self.labels = F.one_hot(torch.Tensor(labels).long()).float()\n",
    "        self.length = len(labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ## TO DO\n",
    "        vectorized_review = self.vectorized_reviews[index]  \n",
    "        label = self.labels[index] \n",
    "\n",
    "        \n",
    "        ##\n",
    "        return vectorized_review, label, index\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac2b530-c711-4130-af39-2d51d61dcbb0",
   "metadata": {},
   "source": [
    "4. **[2 points]** Écrivez une fonction get_dataloader qui prend en input un ensemble de données tokenisé, un vocabulaire de ngrams, la taille des batchs et un hyperparamètre shuffle. Elle doit utiliser la classe ReviewDataset que vous avez définie précédemment et retourner un objet de type torch.utils.data.DataLoader.\n",
    "\n",
    "   *Write a get_dataloader function which takes as input a tokenized dataset, an ngram vocabulary, a batch size, and a shuffle hyperparameter. It should use the ReviewDataset class you previously defined and return an object of type torch.utils.data.DataLoader.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "610b4a14-b094-4113-8bc4-41c7980725e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(tokenized_reviews:list[tuple[list[str], str]], ngram_vocab:list[tuple], batch_size:int, shuffle:bool):\n",
    "    ## TO DO\n",
    "    dataset = ReviewDataset(tokenized_reviews, ngram_vocab)  \n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    \n",
    "    ##\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d931746c-0733-4ed4-94e5-594bbae1b19d",
   "metadata": {},
   "source": [
    "#### Régression logistique avec un réseau neuronal simple\n",
    "\n",
    "Voici notre classe modèle LR que nous utiliserons comme base pour nos classificateurs LR ngrams.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9443eeb9-3b09-4639-b7ea-0965d78cad10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905af248-186f-4e0f-b57f-55b500be0330",
   "metadata": {},
   "source": [
    "5. **[4 points]** Complétez la fonction d'apprentissage pour notre classificateur LR ngram. Votre code doit s'assurer d'entraîner le modèle pour le nombre d'epochs spécifié sur toutes les batchs du dataloader.\n",
    "  \n",
    "   *Complete the training function for our LR ngram classifier. Your code should make sure to train the model for the number of epochs specified over all batches in the training dataloader.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4fcfa88f-713e-413d-a3f6-9839b360dead",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LR_ngram_classifier(train_dataset:list[tuple[str, str]], review_classes:list[str], n:int): \n",
    "    lr = 0.001\n",
    "    batch_size = 32\n",
    "    epochs = 5\n",
    "\n",
    "    tokenized_train = [(preprocess(review), review_class) for review, review_class in  train_dataset]\n",
    "    ngram_vocab = get_ngram_vocabulary_maxsize(tokenized_train, n, 30000)\n",
    "\n",
    "    train_dataloader = get_dataloader(tokenized_train, ngram_vocab, batch_size, shuffle=True)\n",
    "\n",
    "    model = LogisticRegression(len(ngram_vocab), len(review_classes)).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    model.train()\n",
    "    ## TO DO\n",
    "    loss_function = nn.CrossEntropyLoss() \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for batch in train_dataloader:\n",
    "            x_batch, y_batch, _ = batch\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(x_batch)\n",
    "\n",
    "            y_batch = torch.argmax(y_batch, dim=1)  \n",
    "            loss = loss_function(predictions, y_batch)  \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            \n",
    "            predicted_labels = torch.argmax(predictions, dim=1)\n",
    "            correct_predictions += (predicted_labels == y_batch).sum().item()\n",
    "            total_samples += y_batch.size(0)\n",
    "\n",
    "        accuracy = correct_predictions / total_samples * 100\n",
    "    \n",
    "            \n",
    "    ## \n",
    "    return ngram_vocab, model   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4b990af1-e7ea-4c92-a81b-e93f1b3d91cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should run for a couple minutes.\n",
    "unigram_vocab, unigram_model = train_LR_ngram_classifier(train_dataset, review_classes, 1)\n",
    "bigram_vocab, bigram_model = train_LR_ngram_classifier(train_dataset, review_classes, 2)\n",
    "trigram_vocab, trigram_model = train_LR_ngram_classifier(train_dataset, review_classes, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4944003-6925-4896-801f-d2ccb9035138",
   "metadata": {},
   "source": [
    "#### Évaluation de nos classificateurs LR \n",
    "Voici la fonction d'évaluation de nos classificateurs. Elle prend les données test ainsi qu'un classificateur LR ngrams et son vocabulaire correspondant et renvoie les prédictions du modèle pour chaque item test.\n",
    "\n",
    "*Here is the evaluation function for our classifiers. It takes the test data as well as some trained LR ngram classifier and its corresponding vocabulary and returns the model predictions for each item in the test data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "919f3a93-3fcd-4308-8b3a-27a8ec7710ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_LR_ngram_classifier(test_dataset:list[tuple[str, str]], ngram_vocab:list[tuple], model):\n",
    "    batch_size = 32\n",
    "    \n",
    "    tokenized_test = [(preprocess(review), review_class) for review, review_class in  test_dataset]\n",
    "    \n",
    "    test_dataloader = get_dataloader(tokenized_test, ngram_vocab, batch_size, shuffle=False)\n",
    "\n",
    "    indexes = []\n",
    "    labels = []\n",
    "    predictions = []\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_vectorized_reviews, batch_labels, batch_indexes in test_dataloader:\n",
    "            x = batch_vectorized_reviews.to(device)\n",
    "            y_pred = model(x)\n",
    "\n",
    "            indexes += batch_indexes.tolist()\n",
    "            labels += batch_labels.tolist()\n",
    "            predictions += y_pred.tolist()\n",
    "    \n",
    "    results = [{'review_id':index, 'class_label':label, 'prediction': pred} for index, label, pred in zip(indexes, labels, predictions)]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "74d1276f-d267-4450-9b06-3a9b642c0330",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_results = eval_LR_ngram_classifier(test_dataset, unigram_vocab, unigram_model)\n",
    "bigram_results = eval_LR_ngram_classifier(test_dataset, bigram_vocab, bigram_model)\n",
    "trigram_results = eval_LR_ngram_classifier(test_dataset, trigram_vocab, trigram_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b16424-0fb5-4df6-a39a-7d94f9c86332",
   "metadata": {},
   "source": [
    "#### Résultats\n",
    "\n",
    "Comparons maintenant les performances relatives de nos classificateurs LR unigram, bigram et trigram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c9671a-109c-4c8a-80f7-5f26004c8b86",
   "metadata": {},
   "source": [
    "6. **[2 points]** Écrivez une fonction qui retourne les scores moyens de précision, recall et F1 pour toutes les classes, ainsi que le accuracy score d'un classificateur. Notez que les étiquettes et prédictions sont d'un type différent que dans le problème 1, assurez-vous de les traiter correctement.\n",
    "  \n",
    "   *Write a function that returns the overall precision, recall, and balanced F1 scores across all classes as well as overall accuracy of a classifier. Note, that labels and predictions are a different type than in problem 1, make sure to handle them correctly.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "043041ae-ae12-4ee2-9326-93128ff7f0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_scores(results:list[dict]):\n",
    "    accuracy = dict([('precision', 0.0),('recall', 0.0), ('f1', 0.0), ('accuracy', 0.0)])\n",
    "    ## TO DO\n",
    "    true_positive = Counter()\n",
    "    false_positive = Counter()\n",
    "    false_negative = Counter()\n",
    "    total_correct = 0\n",
    "    total = len(results)\n",
    "    \n",
    "    for res in results:\n",
    "        true_label = res['class_label'] if isinstance(res['class_label'], int) else torch.argmax(torch.tensor(res['class_label'])).item()\n",
    "        predicted_label = res['prediction'] if isinstance(res['prediction'], int) else torch.argmax(torch.tensor(res['prediction'])).item()\n",
    "        \n",
    "        if true_label == predicted_label:\n",
    "            total_correct += 1\n",
    "            true_positive[true_label] += 1\n",
    "        else:\n",
    "            false_positive[predicted_label] += 1\n",
    "            false_negative[true_label] += 1\n",
    "    \n",
    "    classes = set(true_positive.keys()).union(false_positive.keys()).union(false_negative.keys())\n",
    "    precision = {cls: true_positive[cls] / (true_positive[cls] + false_positive[cls]) if (true_positive[cls] + false_positive[cls]) > 0 else 0.0 for cls in classes}\n",
    "    recall = {cls: true_positive[cls] / (true_positive[cls] + false_negative[cls]) if (true_positive[cls] + false_negative[cls]) > 0 else 0.0 for cls in classes}\n",
    "    f1 = {cls: 2 * (precision[cls] * recall[cls]) / (precision[cls] + recall[cls]) if (precision[cls] + recall[cls]) > 0 else 0.0 for cls in classes}\n",
    "    \n",
    "    accuracy['accuracy'] = total_correct / total if total > 0 else 0.0\n",
    "    accuracy['precision'] = sum(precision.values()) / len(classes) if classes else 0.0\n",
    "    accuracy['recall'] = sum(recall.values()) / len(classes) if classes else 0.0\n",
    "    accuracy['f1'] = sum(f1.values()) / len(classes) if classes else 0.0\n",
    "\n",
    "    \n",
    "    ##\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "99f930ef-a738-44b2-8f3b-58cee9147c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = {'unigram':get_accuracy_scores(unigram_results),\n",
    "                  'bigram':get_accuracy_scores(bigram_results),\n",
    "                  'trigram':get_accuracy_scores(trigram_results)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f6e116af-eccc-4b2f-ae3f-d197fd2a7c06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>unigram</th>\n",
       "      <td>0.892510</td>\n",
       "      <td>0.8925</td>\n",
       "      <td>0.892499</td>\n",
       "      <td>0.8925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bigram</th>\n",
       "      <td>0.868246</td>\n",
       "      <td>0.8675</td>\n",
       "      <td>0.867433</td>\n",
       "      <td>0.8675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trigram</th>\n",
       "      <td>0.812381</td>\n",
       "      <td>0.8075</td>\n",
       "      <td>0.806745</td>\n",
       "      <td>0.8075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         precision  recall        f1  accuracy\n",
       "unigram   0.892510  0.8925  0.892499    0.8925\n",
       "bigram    0.868246  0.8675  0.867433    0.8675\n",
       "trigram   0.812381  0.8075  0.806745    0.8075"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(accuracies, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96397ca8-9387-4c3f-8c67-af6f2075ce86",
   "metadata": {},
   "source": [
    "## QUESTION BONUS [2 points supplémentaires]\n",
    "\n",
    "Comparez les résultats des six classificateurs et écrivez quelques phrases sur ce que vous observez. Comment le changement de l'hyperparamètre **n** affecte-t-il les performances des classificateurs NB par rapport aux classificateurs LR ? Remarquez-vous des différences entre les classements de accuracy et de F1 score, et pourquoi?\n",
    "\n",
    "*Compare all six classifier results and write a couple sentences about what you observe. How does changing the **n** hyperparameter affect performance of NB versus LR classifiers? Do you notice differences between accuracy and F1 score rankings, why might that be?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ca6b0d-f7f7-418f-a135-71af007d2d36",
   "metadata": {},
   "source": [
    "[ VOTRE RÉPONSE ICI ]: On remaque que pour les classificateurs LR ; une augmentation de l'hyperparamètre n entraîne une baisse de la performance (evite overfitting); là où à contrario , dans le cas des classficateurs NB , on assiste à une augmentation de la performance. cela peut s'expliquer car NB analyse chaque n-gramme séparément sans souffrir autant d’overfitting.\n",
    "\n",
    "Dans NB ; l’accuracy et le F1-score augmentent de manière concordante avec donc le modèle gère mieux les classes avec plus de contexte.\n",
    "Dans LR ;l’accuracy baisse légèrement avec le F1-score , donc les erreurs sont plus équilibrées entre faux positifs et faux négatifs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
